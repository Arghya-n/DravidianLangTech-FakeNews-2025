{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom torch.utils.data import Dataset\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T07:40:20.623823Z","iopub.execute_input":"2025-01-26T07:40:20.624105Z","iopub.status.idle":"2025-01-26T07:40:51.770972Z","shell.execute_reply.started":"2025-01-26T07:40:20.624080Z","shell.execute_reply":"2025-01-26T07:40:51.769820Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Load the datasets\ntrain_df = pd.read_csv('/kaggle/input/fakenews/Fake_train.csv')\ntest_df = pd.read_csv('/kaggle/input/fake-test-without-labels/Fake_test_without_labels.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preprocessing\ndef preprocess_text(text):\n    return text.strip()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['text'] = train_df['text'].apply(preprocess_text)\ntest_df['text'] = test_df['text'].apply(preprocess_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encode labels\nlabel_mapping = {'Fake': 0, 'original': 1}\ntrain_df['label'] = train_df['label'].map(label_mapping)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FakeNewsDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx] if self.labels is not None else -1\n        encoding = self.tokenizer(\n            text,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split data\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    train_df['text'].tolist(),\n    train_df['label'].tolist(),\n    test_size=0.2,\n    random_state=42\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained('google/muril-large-cased')\nmodel = AutoModelForSequenceClassification.from_pretrained('google/muril-large-cased', num_labels=2)\n\n# Create datasets\ntrain_dataset = FakeNewsDataset(train_texts, train_labels, tokenizer, max_length=128)\nval_dataset = FakeNewsDataset(val_texts, val_labels, tokenizer, max_length=128)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    logging_dir='./logs',\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_steps=10,\n    save_total_limit=2,\n    load_best_model_at_end=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the model\ntrainer.train()\n\n# Evaluate the model\ndef compute_metrics(pred):\n    predictions, labels = pred\n    preds = predictions.argmax(-1)\n    return {\n        'accuracy': accuracy_score(labels, preds),\n        'f1': f1_score(labels, preds, average='weighted')\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eval_results = trainer.evaluate()\nprint(\"Evaluation Results:\", eval_results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict on test dataset\ntest_dataset = FakeNewsDataset(test_df['text'].tolist(), None, tokenizer, max_length=128)\npredictions = trainer.predict(test_dataset)\ntest_preds = predictions.predictions.argmax(-1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Map predictions back to labels\ntest_df['label'] = test_preds\ntest_df['label'] = test_df['label'].map({0: 'Fake', 1: 'original'})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save predictions to CSV\ntest_df[['Id', 'label']].to_csv('prediction.csv', index=False)\n\nprint(\"Predictions saved to prediction.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}